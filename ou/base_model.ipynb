{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# ML\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metric to evaluation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What causes a more severe accident?\n",
    "# Data dictionary https://www.seattle.gov/Documents/Departments/SDOT/GIS/Collisions_OD.pdf\n",
    "X_org = pd.read_csv(\"/Users/ou/Projects/traffic_collisions_ml_team2/data/Collisions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220436\n",
       "Name: COLDETKEY, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique id identify each accident\n",
    "X_org['INCKEY'].duplicated().value_counts()\n",
    "X_org['COLDETKEY'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'LOCATION'\n",
    "# Drop 'REPORTNO', 'STATUS', EXCEPTRSNCODE, EXCEPTRSNDESC ST_COLCODE\n",
    "# Convert  INCDATE, INCDTTM\n",
    "X = X_org[['SEVERITYDESC', 'ADDRTYPE', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'COLLISIONTYPE', 'PERSONCOUNT',\n",
    "       'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'INJURIES', 'SERIOUSINJURIES',\n",
    "       'FATALITIES', 'INCDATE', 'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE',\n",
    "       'SDOT_COLDESC', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', 'ROADCOND',\n",
    "       'LIGHTCOND', 'PEDROWNOTGRNT', 'SDOTCOLNUM', 'SPEEDING', 'ST_COLCODE',\n",
    "       'ST_COLDESC', 'SEGLANEKEY', 'CROSSWALKKEY', 'HITPARKEDCAR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "num_col = [\"PERSONCOUNT\", \"PEDCOUNT\", \"PEDCYLCOUNT\", \"VEHCOUNT\", \"INJURIES\",\n",
    "    \"SERIOUSINJURIES\", \"FATALITIES\"]\n",
    "num_mask = X.columns.isin(num_col)\n",
    "cat_col = X.columns[~num_mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ou/.virtualenvs/mlenv/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with 0\n",
    "X.loc[:, num_col] = X.loc[:, num_col].apply(lambda x: x.astype(int).fillna(0))\n",
    "X.loc[:, cat_col] = X.loc[:, cat_col].apply(lambda x: x.fillna('MISSING'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "# Apply LabelEncoder to categorical columns\n",
    "X.loc[:, cat_col] = X.loc[:, cat_col].apply(lambda x: le.fit_transform(x.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['SEVERITYDESC'].to_frame().copy()\n",
    "X = X.drop(['SEVERITYDESC'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier;\n",
    "# This time Scikit-Learn did not have to run OvA or OvO \n",
    "# because Random Forest classifiers can directly classify instances into multiple classes.\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.99921618 0.99757843 0.99898271 0.98323944]\n"
     ]
    }
   ],
   "source": [
    "### Classification\n",
    "## ask for precision, recall, accuracy, F1 Score\n",
    "## Precision: precision = TP/ (TP + FP)\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score1 = precision_score(y_test, y_pred, average=None)\n",
    "print(precision_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all numbers \n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See prediction v.s. test\n",
    "df = pd.DataFrame(data=y_pred, columns=['pred'])\n",
    "compare = df.join(y_test.reset_index(), how='outer').drop(['index'], axis=1)\n",
    "compare['diff'] = compare['pred'] - compare['SEVERITYDESC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98019802 0.99994771 0.99738082 0.98495486 0.98476513]\n"
     ]
    }
   ],
   "source": [
    "## recall/ sensitivity/ true positive rate = TP/ (TP + FN)\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score1 = recall_score(y_test, y_pred, average=None)\n",
    "print(recall_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966320246343341\n"
     ]
    }
   ],
   "source": [
    "## accuracy = (TP + TN)/ Total\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score1 = accuracy_score(y_test, y_pred)\n",
    "print(accuracy_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99       0.99958181 0.99747961 0.99191919 0.98400169]\n"
     ]
    }
   ],
   "source": [
    "## F1 Score = F = 2/ (1/precision + 1/recall)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score1 = f1_score(y_test, y_pred, average=None)\n",
    "print(f1_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis strategy, also known as one-vs-all, is implemented in \\nOneVsRestClassifier. The strategy consists in fitting one classifier \\nper class. For each classifier, the class is fitted against all the \\nother classes. In addition to its computational efficiency \\n(only n_classes classifiers are needed), one advantage of this \\napproach is its interpretability. Since each class is represented \\nby one and only one classifier, it is possible to gain knowledge \\nabout the class by inspecting its corresponding classifier. \\nThis is the most commonly used strategy and is a fair default choice.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-vs-All\n",
    "\n",
    "'''\n",
    "This strategy, also known as one-vs-all, is implemented in \n",
    "OneVsRestClassifier. The strategy consists in fitting one classifier \n",
    "per class. For each classifier, the class is fitted against all the \n",
    "other classes. In addition to its computational efficiency \n",
    "(only n_classes classifiers are needed), one advantage of this \n",
    "approach is its interpretability. Since each class is represented \n",
    "by one and only one classifier, it is possible to gain knowledge \n",
    "about the class by inspecting its corresponding classifier. \n",
    "This is the most commonly used strategy and is a fair default choice.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "y_pred1 = OneVsRestClassifier(SGDClassifier(random_state=42)).fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.34837872 0.65578954 0.01265473 0.27835991]\n",
      "[0.         0.46295037 0.44277413 0.18355065 0.08618987]\n",
      "0.4091608929946112\n",
      "[0.         0.39757494 0.52862962 0.02367706 0.1316243 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ou/.virtualenvs/mlenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_score2 = precision_score(y_test, y_pred1, average=None)\n",
    "print(precision_score2)\n",
    "recall_score2 = recall_score(y_test, y_pred1, average=None)\n",
    "print(recall_score2)\n",
    "accuracy_score2 = accuracy_score(y_test, y_pred1)\n",
    "print(accuracy_score2)\n",
    "f1_score2 = f1_score(y_test, y_pred1, average=None)\n",
    "print(f1_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOneVsOneClassifier constructs one classifier per pair of classes. \\nAt prediction time, the class which received the most votes is selected. \\nIn the event of a tie (among two classes with an equal number of votes), \\nit selects the class with the highest aggregate classification confidence \\nby summing over the pair-wise classification \\nconfidence levels computed by the underlying binary classifiers.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-vs-One\n",
    "\n",
    "'''\n",
    "OneVsOneClassifier constructs one classifier per pair of classes. \n",
    "At prediction time, the class which received the most votes is selected. \n",
    "In the event of a tie (among two classes with an equal number of votes), \n",
    "it selects the class with the highest aggregate classification confidence \n",
    "by summing over the pair-wise classification \n",
    "confidence levels computed by the underlying binary classifiers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "y_pred2 = OneVsOneClassifier(SGDClassifier(random_state=42)).fit(X_train, y_train.values.ravel()).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-443325292c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecall_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "precision_score2 = precision_score(y_test, y_pred2, average=None)\n",
    "print(precision_score2)\n",
    "recall_score2 = recall_score(y_test, y_pred2, average=None)\n",
    "print(recall_score2)\n",
    "accuracy_score2 = accuracy_score(y_test, y_pred2)\n",
    "print(accuracy_score2)\n",
    "f1_score2 = f1_score(y_test, y_pred2, average=None)\n",
    "print(f1_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
